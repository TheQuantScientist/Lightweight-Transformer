{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625a5fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhbarebear/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([1, 64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/anhbarebear/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([49, 1])) that is different to the input size (torch.Size([1, 49, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/anhbarebear/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([56, 1])) that is different to the input size (torch.Size([1, 56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 0.13284273286887877, Validation Loss: 0.0013941121578682214\n",
      "Validation loss decreased (inf --> 0.001394).  Saving model ...\n",
      "Epoch 2: Training Loss: 0.017456996293974594, Validation Loss: 0.0011422374082030728\n",
      "Validation loss decreased (0.001394 --> 0.001142).  Saving model ...\n",
      "Epoch 3: Training Loss: 0.011149759109223813, Validation Loss: 0.004171446285909042\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 4: Training Loss: 0.009594362478335979, Validation Loss: 0.0036762083182111382\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 5: Training Loss: 0.009706445221959705, Validation Loss: 0.0008307103489642031\n",
      "Validation loss decreased (0.001142 --> 0.000831).  Saving model ...\n",
      "Epoch 6: Training Loss: 0.00684874633353361, Validation Loss: 0.0023269105731742457\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7: Training Loss: 0.007136601908788295, Validation Loss: 0.0014697871592943557\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8: Training Loss: 0.012026110172114322, Validation Loss: 0.0007242061656143051\n",
      "Validation loss decreased (0.000831 --> 0.000724).  Saving model ...\n",
      "Epoch 9: Training Loss: 0.0070959211287783906, Validation Loss: 0.02045061788521707\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 10: Training Loss: 0.007309937524989667, Validation Loss: 0.0056625003344379365\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 11: Training Loss: 0.007594186983163088, Validation Loss: 0.0021191625419305637\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 12: Training Loss: 0.007442987110780578, Validation Loss: 0.0010606203868519515\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 13: Training Loss: 0.005113062368456642, Validation Loss: 0.0008806402620393783\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Test Loss: 0.0007242061656143051\n",
      "MAE: 0.020168324932456017\n",
      "RMSE: 0.027001887559890747\n"
     ]
    }
   ],
   "source": [
    "#Transformer \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('IBM.csv', parse_dates=['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the Close prices\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Variable to control the number of future steps to predict\n",
    "steps = 1\n",
    "\n",
    "# Data Preprocessing Function modified for multi-step\n",
    "def create_sequences(data, sequence_length, steps):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - sequence_length - steps + 1):\n",
    "        x = data[i:(i + sequence_length)]\n",
    "        y = data[i + sequence_length:i + sequence_length + steps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Create sequences with modified function\n",
    "sequence_length = 1  # Updated sequence length\n",
    "X, y = create_sequences(price_data, sequence_length, steps)\n",
    "\n",
    "# Split the data\n",
    "split = int(len(X) * 0.9)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Convert to PyTorch tensors, adjusted for multi-step\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader instances, adjusted for multi-step\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the PositionalEncoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the TransformerModel class\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, d_model, n_heads, n_hidden, n_layers, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "        self.decoder = nn.Linear(d_model, n_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1).transpose(0, 1)  # Add a feature dimension and transpose\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(x):\n",
    "            mask = self._generate_square_subsequent_mask(len(x)).to(x.device)\n",
    "            self.src_mask = mask\n",
    "        x = self.pos_encoder(x)\n",
    "        output = self.transformer_encoder(x, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output[-steps:]\n",
    "\n",
    "# Instantiate the model, adjusted for multi-step\n",
    "n_features = steps  # Adjust the output features for multi-step\n",
    "model = TransformerModel(n_features=n_features, d_model=128, n_heads=8, n_hidden=512, n_layers=4, dropout=0.1)\n",
    "\n",
    "# Move model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Scheduler - OneCycleLR\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=50)\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "        \n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, scheduler, epochs, patience):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            sequences, labels = batch\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            predictions = model(sequences)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        validation_loss = evaluate(model, test_loader, criterion)\n",
    "        print(f'Epoch {epoch+1}: Training Loss: {total_loss/len(train_loader)}, Validation Loss: {validation_loss}')\n",
    "        \n",
    "        early_stopping(validation_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # Load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            sequences, labels = batch\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            predictions = model(sequences)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Run the training loop\n",
    "train_model(model, train_loader, test_loader, optimizer, criterion, scheduler, epochs=50, patience=5)\n",
    "\n",
    "# Load best model, adjusted for multi-step\n",
    "best_model = TransformerModel(n_features=n_features, d_model=128, n_heads=8, n_hidden=512, n_layers=4, dropout=0.1)\n",
    "best_model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "best_model.to(device)\n",
    "\n",
    "# Evaluate on test data, adjusted for multi-step\n",
    "test_loss = evaluate(best_model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Calculate and print additional metrics, adjusted for multi-step\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequences, labels = batch\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        predictions = best_model(sequences)\n",
    "        # Adjust shape for multi-step predictions\n",
    "        y_pred.extend(predictions.view(-1).cpu().numpy())\n",
    "        y_true.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "# Convert predictions and true values to arrays\n",
    "y_pred = np.array(y_pred).reshape(-1, steps)\n",
    "y_true = np.array(y_true).reshape(-1, steps)\n",
    "\n",
    "# Calculate MAE and RMSE for each step and then average (if multi-step)\n",
    "mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "print(f'MAE: {np.mean(mae)}')\n",
    "print(f'RMSE: {np.mean(rmse)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
